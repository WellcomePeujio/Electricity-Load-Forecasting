{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "14JrrsJZfLbJyHdROlC1NdU-pNKgn6Lua",
      "authorship_tag": "ABX9TyPfeqzdw537rfrqoWCwvV5O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WellcomePeujio/Electricity-Load-Forecasting/blob/main/ELECTRICITY_LOAD_FORECASTING_IN_BAJA_CALIFORNIA_SUR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview**\n",
        "\n",
        "Precise demand forecasting is crucial for efficient and sustainable management of electrical networks in the changing energy consumption scenario.   This study focuses on the Baja California Sur (BCS) power system, which is a distinctive and crucial case within the Mexican energy industry.   Baja California Sur, due to its pronounced geographical isolation, remains unconnected to Mexico's National Interconnected System (SNI).   The detachment not only presents distinct issues but also increases the electricity expenses in the area, emphasizing the significance of precise demand prediction.\n",
        "\n",
        "The main aim of this study is to create a strong and thorough model for predicting the entire amount of electricity needed in Baja California Sur.   The research incorporates a range of advanced statistical and machine learning techniques to address the complex nature of electricity consumption.   The models used in this analysis include of ARIMA, Linear Regression, Exponential Smoothing Model, Support Vector Regression (SVR), Random Forest Regression, Gradient Boosting Regressor, Artificial Neural Network (ANN), Long Short-Term Memory (LSTM), and a hybrid technique that combines Linear Regression and Random Forest.   Every model provides a distinct viewpoint, presenting a comprehensive comprehension of demand dynamics.\n",
        "\n",
        "One-hot encoding is a crucial step in the modeling process, since it allows for the proper handling of categorical variables like day of the week, holidays, and months.   This strategy improves the model's capacity to capture seasonal and weekly patterns, which are essential in analyzing energy use.\n",
        "\n",
        "The study utilizes stringent performance metrics - Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) - to assess and compare the effectiveness of each model.   These measures will offer a thorough comprehension of the models' precision and dependability in predicting electricity consumption in Baja California Sur.\n",
        "\n",
        "This project seeks to make a substantial contribution to the optimization of Baja California Sur's power system by combining sophisticated forecasting tools and a targeted geographical analysis.   The results are expected to give significant insights for policymakers, energy suppliers, and stakeholders in the region, guiding them towards more efficient and cost-effective electricity management.\n",
        "\n",
        "\n",
        "# Secci√≥n nueva"
      ],
      "metadata": {
        "id": "oedp6XaH0Qrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "Here's a general plan:\n",
        "\n",
        "\n",
        "\n",
        "1.   Loading the Data: We'll start by importing necessary libraries and loading the dataset.\n",
        "2.  Basic Data Overview: Checking the shape, datatypes, and basic statistics of the dataset.\n",
        "3.   Handling Missing Values: Identify and handle any missing values.\n",
        "4.   Visualizations: Visualize distributions, relationships, and patterns in the data.\n",
        "5.   Correlation Analysis: Check the correlation between numerical variables.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rT33iYjg8ibU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/BaseBCS19y20.csv')\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "data.head()"
      ],
      "metadata": {
        "id": "M7s6x5TV-hxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contain the following columns:\n",
        "\n",
        "RealDate: Date and hour in the format 'DD/MM/YYYY HH:MM'.\n",
        "Total_Demand: Numerical value representing some kind of demand.\n",
        "CDD: Numerical value.\n",
        "HDD: Numerical value.\n",
        "Average_Pml: Numerical value.\n",
        "Day_week: Numeric representation of the day of the week (e.g., 1 for Monday, 2 for Tuesday, etc.).\n",
        "Month: Numeric representation of the month.\n",
        "Breakpoint: Numeric value.\n",
        "Holiday: Numeric indicator for holidays (-1 seems to represent non-holidays, but we'll need to confirm)."
      ],
      "metadata": {
        "id": "TAbwHmPq-r8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import Necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# 2. Load the Data\n",
        "data_url = \"/content/BaseBCS19y20.csv\"\n",
        "data = pd.read_csv(data_url)\n",
        "\n",
        "# 3. Basic Data Overview\n",
        "print(data.head())\n",
        "print(\"\\nData Info:\")\n",
        "print(data.info())\n",
        "\n",
        "# 4. Missing Data Check\n",
        "missing_data = data.isnull().sum()\n",
        "print(\"\\nMissing Data:\")\n",
        "print(missing_data)\n",
        "\n",
        "# 5. Summary Statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(data.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "LFCr7a7nAhY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Histograms for Each Numeric Feature:**\n",
        "These histograms show the distribution of values for each numeric feature in the dataset. The kernel density estimation (KDE) line gives a smoothed representation of the distribution. This visualization helps in understanding the central tendency, spread, and skewness of the data. For instance:\n",
        "\n",
        "- The `Total_Demand` histogram might reveal the most common electricity demand levels and any potential outliers.\n",
        "- The `CDD` and `HDD` histograms can offer insights into the typical temperature-related cooling or heating demands, respectively.\n",
        "- The `Average_Pml` histogram can shed light on the general levels of this feature and its variation.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Boxplot of Total_Demand:**\n",
        "The boxplot provides a graphical representation of the distribution of `Total_Demand`. The central box represents the values from the lower to upper quartile (25th to 75th percentile). The middle line inside the box is the median (50th percentile). Whiskers extend from the box to show the range of the data, and dots outside the whiskers might indicate outliers. This plot is useful for identifying the central tendency, variability, and potential outliers in the electricity demand.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Average Total_Demand by Day of the Week:**\n",
        "This bar chart depicts the average electricity demand for each day of the week. By analyzing this, one can infer which days have the highest and lowest demands. For instance, weekdays might have different demand patterns compared to weekends due to industrial or commercial activities.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Average Total_Demand by Month:**\n",
        "This bar chart presents the average electricity demand for each month. This can provide insights into seasonal variations in electricity demand. For example, certain months might have higher demands due to increased heating or cooling needs.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Correlation Heatmap:**\n",
        "The heatmap displays the correlation coefficients between all numeric variables in the dataset. A correlation coefficient measures the strength and direction of a linear relationship between two variables. The values range between -1 (perfect negative correlation) and 1 (perfect positive correlation), with 0 indicating no correlation. This heatmap can help in understanding the relationships between different features. For instance, a strong positive correlation between `CDD` and `Total_Demand` might suggest that as cooling demands increase, the total electricity demand also rises.\n",
        "\n",
        "---\n",
        "\n",
        "Using these visualizations, one can derive valuable insights about the electricity system of Baja California Sur. The patterns and relationships observed can guide decision-making, planning, and forecasting for electricity providers and stakeholders in the region."
      ],
      "metadata": {
        "id": "i_PADwHGCU8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Visualization\n",
        "\n",
        "## Histograms for Each Numeric Feature\n",
        "for column in data.select_dtypes(include=[np.number]).columns:\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.histplot(data[column], kde=True, color=\"skyblue\", bins=30)\n",
        "    plt.title(f\"Distribution of {column}\", fontsize=15)\n",
        "    plt.xlabel(column, fontsize=12)\n",
        "    plt.ylabel(\"Frequency\", fontsize=12)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fTpourXkArxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplot of Total_Demand:\n",
        "\n",
        "The boxplot provides a concise visual summary of the distribution of Total_Demand in the Baja California Sur electricity system.\n",
        "\n",
        "Central Box: The main box represents the interquartile range (IQR):\n",
        "\n",
        "The bottom of the box is the first quartile (Q1), representing the 25th percentile of the data.\n",
        "The top of the box is the third quartile (Q3), representing the 75th percentile.\n",
        "The band inside the box is the median (Q2), which is the 50th percentile. This value divides the dataset into two halves.\n",
        "Whiskers: These are the lines that extend from the top and bottom of the box:\n",
        "\n",
        "The top whisker extends to the highest value within\n",
        "ÔøΩ\n",
        "3\n",
        "+\n",
        "1.5\n",
        "√ó\n",
        "IQR\n",
        "Q3+1.5√óIQR.\n",
        "The bottom whisker extends to the lowest value within\n",
        "ÔøΩ\n",
        "1\n",
        "‚àí\n",
        "1.5\n",
        "√ó\n",
        "IQR\n",
        "Q1‚àí1.5√óIQR.\n",
        "Outliers: Any data points that fall above or below the whiskers are considered outliers. These are typically represented by dots outside the whiskers. They indicate unusual values that may need further investigation.\n",
        "\n",
        "Color: The light coral color provides a clear visual representation, making it easier to discern the different components of the boxplot.\n",
        "\n",
        "Interpretation:\n",
        "From this boxplot, one can glean insights into the central tendency, spread, and potential outliers in the electricity demand for Baja California Sur. The median line gives an idea of the typical electricity demand, while the IQR provides a sense of the variability in demand. Outliers may indicate specific days or times with exceptionally high or low electricity demands, perhaps due to events, faults, or other unusual circumstances.\n",
        "\n"
      ],
      "metadata": {
        "id": "s7Ol7FpRDNVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Boxplot for Total_Demand\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.boxplot(data=data, y='Total_Demand', color=\"lightcoral\")\n",
        "plt.title(\"Boxplot of Total_Demand\", fontsize=15)\n",
        "plt.ylabel(\"Total_Demand\", fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WUjvStDDAv4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame and it has columns 'Total_Demand' and 'Day_of_Week'\n",
        "avg_demand_by_day = df.groupby('Day_week')['Total_Demand'].mean()\n",
        "\n",
        "# Now you can plot the bar chart\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.barplot(x=avg_demand_by_day.index, y=avg_demand_by_day.values, palette=\"viridis\")\n",
        "plt.title(\"Average Total_Demand by Day of the Week\", fontsize=15)\n",
        "plt.ylabel(\"Average Total_Demand\", fontsize=12)\n",
        "plt.xlabel(\"Day of the Week\", fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "90XCeIXW1lz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart represents the average total electricity demand by month. The x-axis indicates the months of the year, from 1 to 12, corresponding to January through December. The y-axis shows the average total demand in megawatt-hours (MWh).\n",
        "\n",
        "Each bar represents a month, with the height of the bar indicating the average demand for that month.\n",
        "\n",
        "From the chart, we can observe that the demand starts at a certain level in January, remains fairly steady through April, and then increases from May onwards, peaking around July or August. After this peak, there is a gradual decrease in average demand, with the lowest demand occurring in December.\n",
        "\n",
        "This seasonal trend might suggest higher electricity usage in the summer months, which could be due to higher temperatures leading to increased use of cooling systems. Conversely, the lower demand towards the end of the year might reflect milder temperatures and less need for air conditioning.\n",
        "\n"
      ],
      "metadata": {
        "id": "mqh43h5W3pdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame and it has columns 'Total_Demand' and 'Month'\n",
        "avg_demand_by_month = df.groupby('Month')['Total_Demand'].mean()\n",
        "\n",
        "# Now you can plot the bar chart\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.barplot(x=avg_demand_by_month.index, y=avg_demand_by_month.values, palette=\"mako\")\n",
        "plt.title(\"Average Total_Demand by Month\", fontsize=15)\n",
        "plt.ylabel(\"Average Total_Demand\", fontsize=12)\n",
        "plt.xlabel(\"Month\", fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d35ftUPk2kQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LzAVrFyo3cmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Heatmap:\n",
        "\n",
        "The heatmap provides a visual representation of the pairwise correlations between the numeric variables in the dataset.\n",
        "\n",
        "Color Scale: The color gradient, ranging from cool blue (indicating -1) to warm red (indicating 1), represents the strength and direction of the correlation:\n",
        "\n",
        "A value close to 1 (warm red) indicates a strong positive correlation: as one variable increases, the other also tends to increase.\n",
        "A value close to -1 (cool blue) shows a strong negative correlation: as one variable increases, the other tends to decrease.\n",
        "A value close to 0 (neutral colors) suggests little to no linear correlation between the variables.\n",
        "Annotations: The numerical values inside the heatmap's cells represent the actual correlation coefficients. These coefficients quantify the strength and direction of the linear relationships.\n",
        "\n",
        "Diagonal: The diagonal from the top left to the bottom right represents the correlation of each variable with itself, which is always 1. Hence, it's uniformly colored.\n",
        "\n",
        "Symmetry: The heatmap is symmetric about its main diagonal, meaning the upper triangular and lower triangular parts are mirror images. This is because the correlation between variables\n",
        "ÔøΩ\n",
        "A and\n",
        "ÔøΩ\n",
        "B is the same as between\n",
        "ÔøΩ\n",
        "B and\n",
        "ÔøΩ\n",
        "A.\n",
        "\n",
        "Interpretation:\n",
        "From this heatmap, stakeholders can quickly identify which variables in the electricity system are strongly related. For instance, if Total_Demand and CDD have a high positive correlation, it suggests that as cooling degree days (indicative of warmer temperatures) increase, the electricity demand also tends to rise. Conversely, a negative correlation between two variables would indicate an inverse relationship. Identifying such relationships is crucial for forecasting, planning, and managing the electricity system efficiently. It can also hint at underlying factors affecting demand and provide insights for further detailed analysis or modeling."
      ],
      "metadata": {
        "id": "ms5x1TovEB9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Correlation Heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame and you want to compute the correlation matrix for it\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Correlation Heatmap\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RW5ejuFZA6Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up the Seaborn style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Load the Data\n",
        "data_url = \"/content/BaseBCS19y20.csv\"\n",
        "data = pd.read_csv(data_url)\n",
        "data['RealDate'] = pd.to_datetime(data['RealDate'])\n",
        "\n",
        "# Visualization\n",
        "\n",
        "# Total Load Demand for One Day\n",
        "plt.figure(figsize=(14, 6))\n",
        "one_day_data = data[data['RealDate'].dt.date == data['RealDate'].dt.date.min()]\n",
        "sns.lineplot(x='RealDate', y='Total_Demand', data=one_day_data)\n",
        "plt.title(\"Total Load Demand for One Day\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Total Load Demand for One Week (7 days)\n",
        "plt.figure(figsize=(14, 6))\n",
        "one_week_data = data[data['RealDate'] < data['RealDate'].min() + pd.Timedelta(days=7)]\n",
        "sns.lineplot(x='RealDate', y='Total_Demand', data=one_week_data)\n",
        "plt.title(\"Total Load Demand for One Week\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Total Load Demand for One Month\n",
        "plt.figure(figsize=(14, 6))\n",
        "one_month_data = data[data['RealDate'].dt.month == data['RealDate'].dt.month.min()]\n",
        "sns.lineplot(x='RealDate', y='Total_Demand', data=one_month_data)\n",
        "plt.title(\"Total Load Demand for One Month\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Total Load Demand for One Year\n",
        "plt.figure(figsize=(14, 6))\n",
        "one_year_data = data[data['RealDate'].dt.year == data['RealDate'].dt.year.min()]\n",
        "sns.lineplot(x='RealDate', y='Total_Demand', data=one_year_data)\n",
        "plt.title(\"Total Load Demand for One Year\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Total Load Demand for Holidays\n",
        "plt.figure(figsize=(14, 6))\n",
        "holiday_data = data[data['Holiday'] == 1]  # Assuming 1 indicates a holiday\n",
        "sns.lineplot(x='RealDate', y='Total_Demand', data=holiday_data)\n",
        "plt.title(\"Total Load Demand for Holidays\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EwfUZSIUTZvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical features"
      ],
      "metadata": {
        "id": "iqmriiJIdwmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the CSV file and take a look at the first few rows."
      ],
      "metadata": {
        "id": "WiDPQwvZVFLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"/content/BaseBCS19y20.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "e1OE-uHfUy33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by cleaning and formatting the RealDate column. We'll convert it to a proper datetime format and inspect any potential discrepancies or issues with the date-time data."
      ],
      "metadata": {
        "id": "mL9YKtosVL_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'RealDate' column to a proper datetime format\n",
        "df['RealDate'] = pd.to_datetime(df['RealDate'], errors='coerce')\n",
        "\n",
        "# Check for any missing or NaT values in the 'RealDate' column\n",
        "missing_dates = df['RealDate'].isna().sum()\n",
        "\n",
        "missing_dates\n"
      ],
      "metadata": {
        "id": "p9IX0T9dVAqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's inspect the dataset for any other potential issues or anomalies, such as missing values in other columns or duplicate rows."
      ],
      "metadata": {
        "id": "iU_HrJf7VXou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in all columns\n",
        "missing_values = df.isna().sum()\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated().sum()\n",
        "\n",
        "missing_values, duplicates\n"
      ],
      "metadata": {
        "id": "NNRjxU0KVgwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's inspect the unique values in these columns to confirm their categorical nature."
      ],
      "metadata": {
        "id": "BlKaESFtVpf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the unique values in potential categorical columns\n",
        "unique_values = {\n",
        "    \"Day_week\": df[\"Day_week\"].unique(),\n",
        "    \"Month\": df[\"Month\"].unique(),\n",
        "    \"Breakpoint\": df[\"Breakpoint\"].unique(),\n",
        "    \"Holiday\": df[\"Holiday\"].unique()\n",
        "}\n",
        "\n",
        "unique_values\n"
      ],
      "metadata": {
        "id": "5IHuHy4MVtmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can proceed with one-hot encoding for these categorical columns. Let's do that now."
      ],
      "metadata": {
        "id": "zOmmw8fwV0ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the identified categorical columns\n",
        "df_encoded = pd.get_dummies(df, columns=[\"Day_week\", \"Month\", \"Breakpoint\", \"Holiday\"], prefix=[\"Day\", \"Month\", \"Breakpoint\", \"Holiday\"])\n",
        "\n",
        "# Display the first few rows of the encoded DataFrame\n",
        "df_encoded.head()\n"
      ],
      "metadata": {
        "id": "RUyC4Z8WV-lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA MODEL"
      ],
      "metadata": {
        "id": "2Xaw4P54WC0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by performing the Augmented Dickey-Fuller test on the Total_Demand column, which seems to be the primary time series data in the dataset."
      ],
      "metadata": {
        "id": "0pX1CS1XWLVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Perform the Augmented Dickey-Fuller test on the 'Total_Demand' column\n",
        "result = adfuller(df_encoded['Total_Demand'])\n",
        "\n",
        "# Extract the test statistic and p-value from the result\n",
        "adf_statistic = result[0]\n",
        "p_value = result[1]\n",
        "\n",
        "adf_statistic, p_value\n"
      ],
      "metadata": {
        "id": "sowJ3I9gWOmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start with the visual inspection of the time series."
      ],
      "metadata": {
        "id": "zlUAqA2dWbQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the 'Total_Demand' time series\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(df_encoded['RealDate'], df_encoded['Total_Demand'], label='Total Demand')\n",
        "plt.title('Total Demand Time Series')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dl_A-7laWc2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll use a window of 24 hours for our rolling calculations."
      ],
      "metadata": {
        "id": "QKn944S0Wpes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the rolling mean and variance with a window of 24 hours\n",
        "rolling_mean = df_encoded['Total_Demand'].rolling(window=24).mean()\n",
        "rolling_var = df_encoded['Total_Demand'].rolling(window=24).var()\n",
        "\n",
        "# Plot the original time series, rolling mean, and rolling variance\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(df_encoded['RealDate'], df_encoded['Total_Demand'], label='Total Demand', color='blue')\n",
        "plt.plot(df_encoded['RealDate'], rolling_mean, label='Rolling Mean', color='red', linestyle='dashed')\n",
        "plt.plot(df_encoded['RealDate'], rolling_var, label='Rolling Variance', color='green', linestyle='dashed')\n",
        "plt.title('Total Demand with Rolling Mean & Variance')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5D4AJslDWngs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with the differencing to make the series stationary. We'll apply first-order differencing and then test again for stationarity. If it's still non-stationary, we might consider higher-order differencing."
      ],
      "metadata": {
        "id": "f_EELornX9er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply first-order differencing\n",
        "df_encoded['Total_Demand_diff'] = df_encoded['Total_Demand'].diff()\n",
        "\n",
        "# Drop the NaN value that results from differencing\n",
        "df_encoded.dropna(inplace=True)\n",
        "\n",
        "# Perform the Augmented Dickey-Fuller test on the differenced series\n",
        "result_diff = adfuller(df_encoded['Total_Demand_diff'])\n",
        "\n",
        "# Extract the test statistic and p-value from the result\n",
        "adf_statistic_diff = result_diff[0]\n",
        "p_value_diff = result_diff[1]\n",
        "\n",
        "adf_statistic_diff, p_value_diff\n"
      ],
      "metadata": {
        "id": "dRDSVVCuX_Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate and analyze the ACF and PACF plots for the differenced series."
      ],
      "metadata": {
        "id": "G6iXknCmYFKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Plot ACF and PACF\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n",
        "\n",
        "# Plot ACF\n",
        "plot_acf(df_encoded['Total_Demand_diff'], lags=50, ax=axes[0])\n",
        "\n",
        "# Plot PACF\n",
        "plot_pacf(df_encoded['Total_Demand_diff'], lags=50, ax=axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xq1VR_daYK_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We will proceed with fitting the ARIMA(1,1,1) model to the Total_Demand time series. Let's perform the model fitting and then summarize the results."
      ],
      "metadata": {
        "id": "IVfcAi_-ZA0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Fit the ARIMA(1,1,1) model\n",
        "model = ARIMA(df_encoded['Total_Demand'], order=(1,1,1))\n",
        "results = model.fit()\n",
        "\n",
        "# Summarize the model results\n",
        "model_summary = results.summary()\n",
        "\n",
        "model_summary\n"
      ],
      "metadata": {
        "id": "_W2J1k-dZEBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's proceed with the diagnostics for the ARIMA(1,1,1) model.\n",
        "\n",
        "We'll conduct the following diagnostics:\n",
        "\n",
        "Residual Plot: Visual inspection of residuals to see if they seem to be white noise.\n",
        "Histogram of Residuals: To check if the residuals are normally distributed.\n",
        "ACF plot of Residuals: To verify if there's any autocorrelation left in the residuals.\n",
        "Let's begin with these diagnostics."
      ],
      "metadata": {
        "id": "nZreCXHhZSqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract residuals\n",
        "residuals = results.resid\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(14, 10))\n",
        "\n",
        "# Plot residuals\n",
        "axes[0].plot(residuals, color='blue')\n",
        "axes[0].set_title('Residuals')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot histogram of residuals\n",
        "axes[1].hist(residuals, bins=50, color='blue', alpha=0.7)\n",
        "axes[1].set_title('Histogram of Residuals')\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot ACF of residuals\n",
        "plot_acf(residuals, lags=50, ax=axes[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wIM9qnljZkM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kp0pN1RI8gJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model's performance using RMSE (Root Mean Squared Error), MAPE (Mean Absolute Percentage Error), and MAE (Mean Absolute Error)"
      ],
      "metadata": {
        "id": "oydxwBU3cgEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this analysis, I'll split the data into an 80-20 split, with 80% of the data used for training and 20% for testing."
      ],
      "metadata": {
        "id": "RZ71dqGgcl3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Split the data into train and test sets (80-20 split)\n",
        "train_size = int(len(df_encoded) * 0.8)\n",
        "train, test = df_encoded['Total_Demand'].iloc[:train_size], df_encoded['Total_Demand'].iloc[train_size:]\n",
        "\n",
        "# Fit the ARIMA(1,1,1) model on the training data\n",
        "model_train = ARIMA(train, order=(1,1,1))\n",
        "results_train = model_train.fit()\n",
        "\n",
        "# Forecast on the test set\n",
        "forecast = results_train.forecast(steps=len(test))\n",
        "\n",
        "# Calculate error metrics\n",
        "rmse = mean_squared_error(test, forecast, squared=False)\n",
        "mae = mean_absolute_error(test, forecast)\n",
        "mape = 100 * (mae / test).mean()\n",
        "\n",
        "rmse, mape, mae\n"
      ],
      "metadata": {
        "id": "BC6tdYQZcoud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll plot the following:\n",
        "\n",
        "Training Data: The portion of the data on which the model was trained.\n",
        "Actual Test Data: The true values from the test set.\n",
        "Forecasted Data: The values predicted by the ARIMA model for the test set."
      ],
      "metadata": {
        "id": "UbJpwYXec9TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training data, actual test data, and forecasts\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot training data\n",
        "plt.plot(train.index, train, label='Training Data', color='blue')\n",
        "\n",
        "# Plot actual test data\n",
        "plt.plot(test.index, test, label='Actual Test Data', color='green')\n",
        "\n",
        "# Plot forecasted data\n",
        "plt.plot(test.index, forecast, label='ARIMA Forecast', color='red', linestyle='dashed')\n",
        "\n",
        "plt.title('ARIMA Model Forecast vs Actuals')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IAAHyGL9dBay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression"
      ],
      "metadata": {
        "id": "2KGOv-2Je2eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create 24 lagged features\n",
        "for i in range(1, 25):\n",
        "    df_encoded[f'lag_{i}'] = df_encoded['Total_Demand'].shift(i)\n",
        "\n",
        "# Drop rows with NaN values resulting from lagging\n",
        "df_encoded.dropna(inplace=True)\n",
        "\n",
        "# Split the data into train and test sets (80-20 split)\n",
        "X = df_encoded[['lag_' + str(i) for i in range(1, 25)]]\n",
        "y = df_encoded['Total_Demand']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "rmse_lr = mean_squared_error(y_test, y_pred, squared=False)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred)\n",
        "mape_lr = 100 * (mae_lr / y_test).mean()\n",
        "\n",
        "rmse_lr, mape_lr, mae_lr\n"
      ],
      "metadata": {
        "id": "-1yA22Yue6RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's proceed with the visualization. I'll plot the actual test values against the predicted values from the linear regression model to provide a clearer understanding of the model's forecasting performance."
      ],
      "metadata": {
        "id": "CjrzJPHCf3bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual vs predicted values for the linear regression model\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot actual test data\n",
        "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
        "\n",
        "# Plot predicted data\n",
        "plt.plot(y_test.index, y_pred, label='Linear Regression Predictions', color='red', linestyle='dashed')\n",
        "\n",
        "plt.title('Linear Regression Model: Actual vs Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B7pLXEXaf5mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exponential Smoothing Model"
      ],
      "metadata": {
        "id": "A1GidM_YjE0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exponential Smoothing (often referred to as Holt-Winters Exponential Smoothing) is a popular time series forecasting method that takes into account various components such as level, trend, and seasonality. There are different versions of Exponential Smoothing, including:\n",
        "\n",
        "Simple Exponential Smoothing: Suitable for data with no clear trend or seasonality.\n",
        "Double Exponential Smoothing (Holt's Linear): Suitable for data with a trend but no seasonality.\n",
        "Triple Exponential Smoothing (Holt-Winters): Suitable for data with both trend and seasonality."
      ],
      "metadata": {
        "id": "6fBmLIyJjPlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# Fit the Holt-Winters Exponential Smoothing model\n",
        "hw_model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=24)\n",
        "hw_fit = hw_model.fit()\n",
        "\n",
        "# Forecast on the test set\n",
        "hw_forecast = hw_fit.forecast(steps=len(y_test))\n",
        "\n",
        "# Calculate error metrics\n",
        "rmse_hw = mean_squared_error(y_test, hw_forecast, squared=False)\n",
        "mae_hw = mean_absolute_error(y_test, hw_forecast)\n",
        "mape_hw = 100 * (mae_hw / y_test).mean()\n",
        "\n",
        "rmse_hw, mape_hw, mae_hw\n"
      ],
      "metadata": {
        "id": "llWFtc0sjLIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's proceed with the visualization. I'll plot the actual test values against the predicted values from the Holt-Winters Exponential Smoothing model to provide a clearer understanding of the model's forecasting performance."
      ],
      "metadata": {
        "id": "z9YyuGkBjvnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual vs predicted values for the Holt-Winters model\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot actual test data\n",
        "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
        "\n",
        "# Plot predicted data from Holt-Winters model\n",
        "plt.plot(y_test.index, hw_forecast, label='Holt-Winters Predictions', color='red', linestyle='dashed')\n",
        "\n",
        "plt.title('Holt-Winters Model: Actual vs Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pbWQHpVlfxY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Regression (SVR) Model"
      ],
      "metadata": {
        "id": "BWlKimddnrsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "upport Vector Regression (SVR) is a regression variant of the Support Vector Machine (SVM), a powerful machine learning algorithm. SVR can be effective for time series forecasting, especially when the data has non-linear patterns."
      ],
      "metadata": {
        "id": "MnxCFqi4nupN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scaling the features and target variable\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Train the SVR model\n",
        "svr = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
        "svr.fit(X_train_scaled, y_train_scaled.ravel())\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_scaled = svr.predict(X_test_scaled)\n",
        "\n",
        "# Inverse transform the scaled predictions\n",
        "y_pred_svr = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
        "\n",
        "# Calculate error metrics\n",
        "rmse_svr = mean_squared_error(y_test, y_pred_svr, squared=False)\n",
        "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
        "mape_svr = 100 * (mae_svr / y_test).mean()\n",
        "\n",
        "rmse_svr, mape_svr, mae_svr\n"
      ],
      "metadata": {
        "id": "E023H6umn1ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's proceed with the visualization. I'll plot the actual test values against the predicted values from the Support Vector Regression (SVR) model to provide a clearer understanding of the model's forecasting performance."
      ],
      "metadata": {
        "id": "qZ8fzm0NoMbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual vs predicted values for the SVR model\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot actual test data\n",
        "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
        "\n",
        "# Plot predicted data from SVR model\n",
        "plt.plot(y_test.index, y_pred_svr, label='SVR Predictions', color='red', linestyle='dashed')\n",
        "\n",
        "plt.title('SVR Model: Actual vs Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-sNdSvekoNyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Regression"
      ],
      "metadata": {
        "id": "nzbzGhubqO3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble machine learning algorithm that can be used for both classification and regression tasks."
      ],
      "metadata": {
        "id": "xoqg89nSqRy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Train the Random Forest Regressor model\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "mape_rf = 100 * (mae_rf / y_test).mean()\n",
        "\n",
        "rmse_rf, mape_rf, mae_rf\n"
      ],
      "metadata": {
        "id": "6QfJrvAfqu-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual vs predicted values for the Random Forest Regressor model\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot actual test data\n",
        "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
        "\n",
        "# Plot predicted data from Random Forest model\n",
        "plt.plot(y_test.index, y_pred_rf, label='Random Forest Predictions', color='red', linestyle='dashed')\n",
        "\n",
        "plt.title('Random Forest Regressor Model: Actual vs Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PcN6hJg2q5d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Machines (GBM)"
      ],
      "metadata": {
        "id": "FWzF3pWhs38G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Machines (GBM) is an ensemble machine learning technique that builds an additive model in a forward stage-wise manner. It is a popular and effective approach for regression tasks."
      ],
      "metadata": {
        "id": "II5xGaADs_Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Train the Gradient Boosting Regressor model\n",
        "gbm = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gbm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_gbm = gbm.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "rmse_gbm = mean_squared_error(y_test, y_pred_gbm, squared=False)\n",
        "mae_gbm = mean_absolute_error(y_test, y_pred_gbm)\n",
        "mape_gbm = 100 * (mae_gbm / y_test).mean()\n",
        "\n",
        "rmse_gbm, mape_gbm, mae_gbm\n"
      ],
      "metadata": {
        "id": "GWeGob8FtAny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the actual vs. predicted values for the Gradient Boosting Regressor model."
      ],
      "metadata": {
        "id": "gKpWqGovtZKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual vs predicted values for the Gradient Boosting Regressor model\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot actual test data\n",
        "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
        "\n",
        "# Plot predicted data from Gradient Boosting model\n",
        "plt.plot(y_test.index, y_pred_gbm, label='GBM Predictions', color='red', linestyle='dashed')\n",
        "\n",
        "plt.title('Gradient Boosting Regressor Model: Actual vs Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8qffWbC9tcAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Networks (ANNs)"
      ],
      "metadata": {
        "id": "7sMkAezNI_Ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANN Model"
      ],
      "metadata": {
        "id": "Txh8hkU8BB3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/BaseBCS19y20.csv')\n",
        "\n",
        "# Convert the 'Date' column to datetime format and set it as the index, if it exists\n",
        "if 'Date' in df.columns:\n",
        "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
        "    df.set_index('Date', inplace=True)\n",
        "\n",
        "# One-hot encoding for the categorical 'Hour' column, if it exists\n",
        "if 'Hour' in df.columns:\n",
        "    encoder = OneHotEncoder(sparse=False, drop='first')\n",
        "    encoded_features = encoder.fit_transform(df['Hour'].values.reshape(-1, 1))\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names(['Hour']), index=df.index)\n",
        "    df = pd.concat([df, encoded_df], axis=1)\n",
        "    df.drop('Hour', axis=1, inplace=True)\n",
        "\n",
        "# Create 24 lagged features\n",
        "for i in range(1, 25):\n",
        "    df[f'lag_{i}'] = df['Total_Demand'].shift(i)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Define columns to exclude from X (non-numeric or already processed)\n",
        "exclude_columns = ['Total_Demand', 'RealDate']  # Exclude 'RealDate' column\n",
        "\n",
        "# After creating lagged features and dropping NaN rows, define X and y as:\n",
        "y = df['Total_Demand']\n",
        "X = df.drop(exclude_columns, axis=1)\n",
        "\n",
        "# Ensure that all columns in X are numeric, and convert them if needed\n",
        "X = X.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric values to NaN\n",
        "\n",
        "# Drop rows with NaN values, if any\n",
        "X.dropna(inplace=True)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Scaling the features and target variable\n",
        "scaler_X = MinMaxScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Design the ANN architecture\n",
        "model = Sequential([\n",
        "    Dense(50, activation='relu', input_dim=X_train_scaled.shape[1]),\n",
        "    Dense(25, activation='relu'),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test_scaled), verbose=1)\n"
      ],
      "metadata": {
        "id": "YOQFvBy9A8i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred_scaled_ann = model.predict(X_test_scaled)\n",
        "\n",
        "# Inverse transform the scaled predictions to original scale\n",
        "y_pred_ann = scaler_y.inverse_transform(y_pred_scaled_ann)\n"
      ],
      "metadata": {
        "id": "XVS1mqqFBldg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
      ],
      "metadata": {
        "id": "8xTsnVKFCkIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSE (Root Mean Squared Error)\n",
        "rmse_ann = mean_squared_error(y_test, y_pred_ann, squared=False)\n",
        "\n",
        "# MAE (Mean Absolute Error)\n",
        "mae_ann = mean_absolute_error(y_test, y_pred_ann)\n",
        "\n",
        "# MAPE (Mean Absolute Percentage Error)\n",
        "mape_ann = 100 * (abs(y_test.values - y_pred_ann.flatten()) / y_test.values).mean()\n",
        "\n",
        "print(f\"RMSE: {rmse_ann}\")\n",
        "print(f\"MAE: {mae_ann}\")\n",
        "print(f\"MAPE: {mape_ann}%\")"
      ],
      "metadata": {
        "id": "Hc29V7opBuZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plotting the actual vs predicted values for the ANN model\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot actual test data\n",
        "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
        "\n",
        "# Plot predicted data from ANN model\n",
        "plt.plot(y_test.index, y_pred_ann, label='ANN Predictions', color='red', linestyle='dashed')\n",
        "\n",
        "plt.title('ANN Model: Actual vs Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2Yv6zxL-DSAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "j2Ot2RlKEDJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/BaseBCS19y20.csv')\n",
        "\n",
        "# Create lagged features\n",
        "for i in range(1, 25):\n",
        "    df[f'lag_{i}'] = df['Total_Demand'].shift(i)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Define columns to exclude from X (non-numeric or already processed)\n",
        "exclude_columns = ['Total_Demand', 'RealDate']\n",
        "\n",
        "# Define X and y\n",
        "y = df['Total_Demand']\n",
        "X = df.drop(exclude_columns, axis=1)\n",
        "\n",
        "# Ensure all columns in X are numeric\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "X.dropna(inplace=True)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Scaling the features and target variable\n",
        "scaler_X = MinMaxScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Reshape X data for LSTM (samples, time steps, features)\n",
        "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_lstm, y_train_scaled, epochs=50, batch_size=32, validation_data=(X_test_lstm, y_test_scaled), verbose=1)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_scaled_lstm = model.predict(X_test_lstm)\n",
        "\n",
        "# Inverse transform the scaled predictions to the original scale\n",
        "y_pred_lstm = scaler_y.inverse_transform(y_pred_scaled_lstm)\n",
        "\n",
        "# Calculate error metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Inverse transform the scaled test data to the original scale\n",
        "y_test_original = scaler_y.inverse_transform(y_test_scaled)\n",
        "\n",
        "# Calculate RMSE, MAE, and MAPE\n",
        "rmse_lstm = np.sqrt(mean_squared_error(y_test_original, y_pred_lstm))\n",
        "mae_lstm = mean_absolute_error(y_test_original, y_pred_lstm)\n",
        "mape_lstm = 100 * (mae_lstm / y_test_original).mean()\n",
        "\n",
        "print(f\"RMSE: {rmse_lstm}\")\n",
        "print(f\"MAE: {mae_lstm}\")\n",
        "print(f\"MAPE: {mape_lstm}%\")\n",
        "\n",
        "# Plotting the actual vs predicted values for the LSTM model\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(y_test.index, y_test_original, label='Actual Test Data', color='green')\n",
        "plt.plot(y_test.index, y_pred_lstm, label='LSTM Predictions', color='red', linestyle='dashed')\n",
        "plt.title('LSTM Model: Actual vs Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "65ODvRlZEGJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid model (Linear Regression + Random Forest)"
      ],
      "metadata": {
        "id": "jP_nAReEJCck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Create and train the Linear Regression model\n",
        "linear_reg_model = LinearRegression()\n",
        "linear_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Create and train the Random Forest model\n",
        "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predictions with Linear Regression\n",
        "y_pred_lr = linear_reg_model.predict(X_test)\n",
        "\n",
        "# Predictions with Random Forest\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "\n",
        "# Weighted Average (You can adjust the weights as needed)\n",
        "weight_lr = 0.7  # Weight for Linear Regression\n",
        "weight_rf = 0.3  # Weight for Random Forest\n",
        "\n",
        "# Combine predictions\n",
        "y_pred_hybrid = (weight_lr * y_pred_lr) + (weight_rf * y_pred_rf)\n",
        "\n",
        "\n",
        "# Calculate error metrics for the hybrid model\n",
        "rmse_hybrid = np.sqrt(mean_squared_error(y_test, y_pred_hybrid))\n",
        "mae_hybrid = mean_absolute_error(y_test, y_pred_hybrid)\n",
        "mape_hybrid = 100 * (mae_hybrid / y_test).mean()\n",
        "\n",
        "print(f\"RMSE (Hybrid Model): {rmse_hybrid}\")\n",
        "print(f\"MAE (Hybrid Model): {mae_hybrid}\")\n",
        "print(f\"MAPE (Hybrid Model): {mape_hybrid}%\")\n",
        "\n",
        "\n",
        "# Plotting the actual vs predicted values for the hybrid model\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot actual test data\n",
        "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
        "\n",
        "# Plot predicted data from the hybrid model\n",
        "plt.plot(y_test.index, y_pred_hybrid, label='Hybrid Model Predictions', color='blue', linestyle='dashed')\n",
        "\n",
        "plt.title('Hybrid Model: Actual vs Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Total Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "PhSFhVJYJKq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "\n",
        "The thorough examination of different forecasting models within the framework of the Baja California Sur power system produces enlightening and significant results.   The study primarily aimed to assess the effectiveness of various statistical and machine learning models in accurately predicting electricity demand. This was particularly important due to the distinctive energy situation in the region and the expensive electricity costs resulting from its disconnection from the Sistema Interconectado Nacional (SNI).\n",
        "\n",
        "The findings demonstrate notable discrepancies in the performance of the models.   The Linear Regression + Random Forest hybrid model stands out as the most efficient, showcasing the lowest values across all three crucial metrics: RMSE (6.41), MAE (4.02), and MAPE (1.16%).   The exceptional performance of this hybrid model highlights the effectiveness of integrating linear and non-linear methods to capture the intricate patterns in electricity consumption.\n",
        "\n",
        "Similarly, the Long Short-Term Memory (LSTM) model and the standalone Linear Regression model also demonstrate impressive performance, with consistently low values for RMSE, MAE, and MAPE.   These findings indicate the efficacy of both deep learning and classical regression methods in this field.\n",
        "\n",
        "However, in this particular application, models such as ARIMA and the Exponential Smoothing Model, which are traditionally used for time series forecasting, demonstrate less favorable outcomes.   This observation suggests that the electricity demand in Baja California Sur is complex and non-linear, which current models are not well-suited to address.\n",
        "\n",
        "In summary, the study emphasizes the significance of choosing suitable modeling tools for predicting power consumption in specific regions such as Baja California Sur. Additionally, it establishes a precedent for using hybrid methodologies.   The results have significant ramifications for energy administration in the area, providing avenues for more efficient, economical, and sustainable management of the electrical system.   Policymakers, energy providers, and stakeholders can utilize these observations to enhance efficiency, forecast capacity needs, and develop plans that are in line with the unique energy characteristics of the region.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HWUnHqoW1DJZ"
      }
    }
  ]
}